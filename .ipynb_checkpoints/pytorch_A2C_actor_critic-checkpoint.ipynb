{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random, randint\n",
    "import numpy as np\n",
    "from classes import Bandit\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional\n",
    "from torch.autograd import Variable\n",
    "from math import log\n",
    "\n",
    "class a2c_network(nn.Module):\n",
    "    \"\"\"\n",
    "    This class defines an Advantage Actor-Critic network implemented in Pytorch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, N_arms, state_dim, actor_layers, critic_layers, learning_rate ):\n",
    "        super(a2c_network, self).__init__()\n",
    "        \n",
    "        self.actor = [ nn.Linear(state_dim, actor_layers[0]) ]\n",
    "        for i in range(len(actor_layers)-1):\n",
    "            self.actor.append( nn.Linear(actor_layers[i], actor_layers[i+1]) )\n",
    "        self.actor.append( nn.Linear(actor_layers[-1], N_arms) )\n",
    "        \n",
    "        for i, layer in enumerate(self.actor):\n",
    "            setattr(self, \"actor\"+str(i), layer)\n",
    "        \n",
    "        self.critic = [ nn.Linear(state_dim, critic_layers[0]) ]\n",
    "        for i in range(len(critic_layers)-1):\n",
    "            self.critic.append( nn.Linear(critic_layers[i], critic_layers[i+1]) )\n",
    "        self.critic.append( nn.Linear(critic_layers[-1], 1) )\n",
    "        \n",
    "        for i, layer in enumerate(self.critic):\n",
    "            setattr(self, \"critic\"+str(i), layer)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Returns an expected value and recommended policy for a given state\n",
    "        \"\"\"\n",
    "        policy = functional.relu(self.actor[0](state))\n",
    "        for i in range(1, len(self.actor)-1):\n",
    "            policy = functional.relu( self.actor[i](policy) )\n",
    "        policy = functional.softmax( self.actor[-1](policy), dim=1)\n",
    "        \n",
    "        value = functional.relu(self.critic[0](state))\n",
    "        for i in range(1, len(self.critic)-1):\n",
    "            value = functional.relu( self.critic[i](value) )\n",
    "        value = self.critic[-1](value) \n",
    "        \n",
    "        return value, policy\n",
    "\n",
    "def init_a2c(B, kwargs):\n",
    "    \"\"\"\n",
    "    Initializes an Advantage Actor-Critic (A2C) agent\n",
    "    \"\"\"\n",
    "    B.state = env.reset()\n",
    "    B.state = B.state.reshape(1,B.state.shape[0])\n",
    "    B.state = torch.FloatTensor( B.state )\n",
    "    B.done = False\n",
    "    \n",
    "    B.N_arms = kwargs['N_arms'] \n",
    "    B.lr = kwargs['lr']\n",
    "    B.discount = kwargs['discount']\n",
    "    B.maxlen = kwargs['maxlen']\n",
    "    \n",
    "    B.Q = []\n",
    "    B.logP = []\n",
    "    B.history = []\n",
    "    B.values = []\n",
    "    B.policies = []\n",
    "    B.entropy = 0\n",
    "    \n",
    "    B.model = a2c_network( B.N_arms, B.state.shape[1], kwargs['actor_layers'], kwargs['critic_layers'], kwargs['lr'])\n",
    "    B.optim = optim.Adam( B.model.parameters(), lr = B.lr)\n",
    "    \n",
    "\n",
    "def a2c(B):\n",
    "    \"\"\"\n",
    "    Selects policy from a weighted distribution calculated by the agent from the current state \n",
    "    \"\"\"\n",
    "    value, policy = B.model.forward( B.state )\n",
    "    B.pdist = policy\n",
    "    B.values.append( value.detach().numpy()[0][0] )\n",
    "    B.policies.append( np.squeeze( policy.detach().numpy() ) )\n",
    "    return np.random.choice( B.N_arms, p=B.policies[-1] )\n",
    "\n",
    "\n",
    "def update_a2c(B, arm, reward, observation):\n",
    "    \"\"\"\n",
    "    Updates the agent based on the most recent action and reward using the Advantage Actor-Critic (A2C) method\n",
    "    \"\"\"\n",
    "    observation = observation.reshape(1, observation.shape[0])\n",
    "    observation = torch.FloatTensor(observation)\n",
    "    B.history.append( (B.state, arm, reward, observation) )\n",
    "    B.state = observation\n",
    "    \n",
    "    B.logP.append( torch.log(B.pdist.squeeze(0)[arm]) )\n",
    "    B.entropy -= np.sum( np.mean(B.policies[-1]) * np.log(B.policies[-1]) )\n",
    "    \n",
    "    if B.done or len(B.history) == B.maxlen:\n",
    "        Q0, useless = B.model.forward( observation )\n",
    "        Q0 = Q0.detach().numpy()[0][0]\n",
    "        \n",
    "        N = len(B.history)\n",
    "        B.Q = [ Q0 for n in range(N) ]\n",
    "        for i in range(N-1, 1, -1):\n",
    "            B.Q[i-1] = B.history[i-1][2] + B.Q[i] * B.discount\n",
    "        \n",
    "        logP = torch.stack(B.logP)\n",
    "        Q = torch.FloatTensor(B.Q)\n",
    "        V = torch.FloatTensor( B.values )\n",
    "        \n",
    "        A = Q - V\n",
    "        a_loss = ( -logP * A ).mean()\n",
    "        c_loss = 0.5 * (A.pow(2)).mean()\n",
    "        loss = a_loss + c_loss - 0.001*B.entropy \n",
    "        \n",
    "        B.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        B.optim.step()\n",
    "        \n",
    "        B.Q = []\n",
    "        B.logP = []\n",
    "        B.history = []\n",
    "        B.values = []\n",
    "        B.policies = []\n",
    "        B.entropy = 0\n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The following cell defines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "#env = gym.make(\"CartPole-v0\")\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "\n",
    "kwargs = { \"N_arms\": env.action_space.n, \"discount\": 0.99, \"lr\": 0.0003, \"actor_layers\": [256], \"critic_layers\": [256], \"maxlen\": 10000}\n",
    "\n",
    "# Creates a multi-armed bandit using the defined A2C functions and parameters\n",
    "B = Bandit(env, a2c, update_a2c, init_a2c, kwargs )\n",
    "\n",
    "\n",
    "# Initialize variables to track iterations, rewards and weights\n",
    "counter = 0\n",
    "best_return = -200\n",
    "rewards = []\n",
    "all_weights = []\n",
    "\n",
    "# Sets the number of iterations and interval to print average reward (never prints if set to None)\n",
    "N_iterations = 2500\n",
    "interval = 25\n",
    "\n",
    "for i in range(N_iterations):\n",
    "    \n",
    "    done = False\n",
    "    counter += 1\n",
    "    if interval is not None and counter%interval == 0:\n",
    "        print( counter, \"\\t\", sum(rewards[-interval:])/float(interval) )\n",
    "    total_return = 0\n",
    "    env.reset()\n",
    "    j = 0\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        arm = B.policy(B)\n",
    "        observation, reward, done, info = env.step(arm)\n",
    "        B.done = done\n",
    "        j += 1\n",
    "        B.update(B, arm, reward, observation )\n",
    "        total_return += reward\n",
    "        if done:\n",
    "            rewards.append( total_return )\n",
    "            env.reset()\n",
    "            if total_return > best_return:\n",
    "                best_return = total_return\n",
    "            break\n",
    "    \n",
    "\n",
    "env.close()\n",
    "print( \"Done after \", counter, \" episodes\" )\n",
    "print(best_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N, n = len(rewards), 5\n",
    "rewards2 = [ sum(rewards[i-n:i+n])/float(2*n) for i in range(n,N-n) ]\n",
    "\n",
    "fig, ax = plt.subplots( figsize=(10,5) )\n",
    "ax.plot( rewards )\n",
    "ax.plot( rewards2, c='red' )\n",
    "fig.patch.set_facecolor('#212121')\n",
    "ax.tick_params(colors='white')\n",
    "ax.set_ylim(-500,300)\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell records a video of the (now trained) agent as it attempts a specified number of trials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import wrappers\n",
    "\n",
    "recorder = wrappers.monitoring.video_recorder.VideoRecorder(env, 'C:/Users/Zeyad/Desktop/Reinforcement Learning/videos/lunar_lander_a2c.mp4')\n",
    "\n",
    "N_trials = 1\n",
    "for i in range(N_trials):\n",
    "    done = False\n",
    "    total_return = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        arm = B.policy(B)\n",
    "        observation, reward, done, info = env.step(arm)\n",
    "        B.update(B, arm, reward, observation)\n",
    "        total_return += reward\n",
    "        recorder.capture_frame()\n",
    "        if done:\n",
    "            rewards.append( total_return )\n",
    "            env.reset()\n",
    "            recorder.close()\n",
    "            recorder.Enabled = False\n",
    "            best_return = total_return\n",
    "            break\n",
    "env.close()\n",
    "\n",
    "print( rewards[-10:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

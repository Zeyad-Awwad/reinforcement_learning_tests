{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from random import sample\n",
    "from collections import deque\n",
    "\n",
    "def init_dqn(B, kwargs):\n",
    "    B.N_arms = kwargs['N_arms'] \n",
    "    B.context = env.reset()\n",
    "    B.context = B.context.reshape(1,B.context.shape[0])\n",
    "    B.done = False\n",
    "    \n",
    "    B.alpha = kwargs['alpha']\n",
    "    B.discount = kwargs['discount']\n",
    "    B.eps = kwargs['eps']\n",
    "    B.eps_min = kwargs['eps_min']\n",
    "    B.eps_reduction = kwargs['eps_reduction']\n",
    "    B.batch_size = kwargs['batch_size']\n",
    "    \n",
    "    B.model = Sequential()\n",
    "    B.model.add(Dense(kwargs['layer_size'][0], input_dim=env.observation_space.shape[0], activation='relu'))\n",
    "    for layer_size in kwargs['layer_size'][1:]:\n",
    "        B.model.add(Dense(layer_size, activation='relu'))\n",
    "    B.model.add(Dense(env.action_space.n, activation='linear'))\n",
    "    B.model.compile(loss='mse', optimizer=Adam(lr=kwargs['lr']))\n",
    "    \n",
    "    B.history = deque(maxlen=1000000)\n",
    "    return\n",
    "\n",
    "def dqn(B):\n",
    "    p = random()\n",
    "    if p < B.eps:\n",
    "        return randint(0, B.N_arms-1)\n",
    "    else:\n",
    "        return np.argmax( B.model.predict(B.context) )\n",
    "\n",
    "def update_dqn(B, arm, reward, observation):\n",
    "    observation = observation.reshape(1, observation.shape[0])\n",
    "    B.history.append( (B.context, arm, reward, observation) )\n",
    "    B.context = observation\n",
    "    if True or B.done:\n",
    "        \n",
    "        if len(B.history) < B.batch_size:\n",
    "            return\n",
    "        \n",
    "        minibatch = sample(B.history, B.batch_size)\n",
    "        \n",
    "        states, actions, rewards, observations = zip(*minibatch)\n",
    "        states = np.squeeze(states)\n",
    "        observations = np.squeeze(observations)\n",
    "        actions = np.squeeze(np.asarray(actions))\n",
    "        Q_next = B.model.predict_on_batch( np.stack(observations) )\n",
    "        \n",
    "        Q_new = np.asarray(rewards) + B.discount * np.amax(Q_next, axis=1)\n",
    "        Q_new[-1] = rewards[-1]\n",
    "        Q = B.model.predict_on_batch( np.stack(states) )\n",
    "        Q[np.arange(0,len(actions)), actions] = Q_new\n",
    "        \n",
    "        B.model.fit(states, Q, epochs=1, verbose=0)\n",
    "        if B.eps > B.eps_min:\n",
    "            B.eps *= B.eps_reduction\n",
    "        B.done = False\n",
    "        return    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "#env = gym.make('MountainCar-v0')\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "\n",
    "\n",
    "kwargs = { \"N_arms\": 4, \"alpha\": 0.001, \"layer_size\": [150,120], \"discount\": 0.99, \"eps\": 1.0, \n",
    "           \"eps_min\": 0.01, \"eps_reduction\": 0.998, \"lr\": 0.001, \"batch_size\": 64 }\n",
    "\n",
    "B = Bandit(env, dqn, update_dqn, init_dqn, kwargs )\n",
    "\n",
    "rewards = []\n",
    "all_weights = []\n",
    "counter = 0\n",
    "#for i in range(100000):\n",
    "counter = 0\n",
    "best_return = -200\n",
    "\n",
    "interval = 25\n",
    "\n",
    "for i in range(1000):\n",
    "    done = False\n",
    "    counter += 1\n",
    "    if counter%interval == 0:\n",
    "        print( counter, \"\\t\", sum(rewards[-interval:])/float(interval) )\n",
    "    total_return = 0\n",
    "    env.reset()\n",
    "    for j in range(3000):\n",
    "        #env.render()\n",
    "        arm = B.policy(B)\n",
    "        observation, reward, done, info = env.step(arm)\n",
    "        B.done = done\n",
    "        B.update(B, arm, reward, observation)\n",
    "        total_return += reward\n",
    "        if done:\n",
    "            rewards.append( total_return )\n",
    "            env.reset()\n",
    "            if total_return > best_return:\n",
    "                best_return = total_return\n",
    "            break\n",
    "env.close()\n",
    "print( \"Done after \", counter, \" episodes\" )\n",
    "print(best_return)\n",
    "#plt.plot(rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

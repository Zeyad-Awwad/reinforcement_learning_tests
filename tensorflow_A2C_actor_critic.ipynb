{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Flatten, add, subtract, multiply\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.models import Model\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from random import sample\n",
    "from collections import deque\n",
    "from math import log\n",
    "\n",
    "\n",
    "def a2c_loss(B):\n",
    "    \"\"\"\n",
    "    Defines a loss function that can access the bandit B while conforming to Keras's expected inputs (only y_true and y_pred)\n",
    "    \"\"\"\n",
    "    \n",
    "    def loss_function(y_true,y_pred):\n",
    "        K.get_variable_shape(y_pred)\n",
    "        return K.variable(B.loss)\n",
    "\n",
    "    return loss_function\n",
    "\n",
    "def a2c_actor_loss(B):\n",
    "    \"\"\"\n",
    "    Defines a loss function for the actor that can access the bandit B while conforming to Keras's expected inputs (only y_true and y_pred)\n",
    "    \"\"\"\n",
    "    \n",
    "    def loss_function(y_true,y_pred): \n",
    "        if len(B.advantage) == 0: \n",
    "            answer = np.ones( (1,1), dtype = np.float32 )\n",
    "        else:\n",
    "            B.advantage = B.Q - np.asarray(B.values)         \n",
    "            logP = np.stack( B.logP )\n",
    "            B.actor_loss = np.mean( -logP * B.advantage )\n",
    "            answer = B.actor_loss\n",
    "        return tf.convert_to_tensor( answer )\n",
    "\n",
    "    return loss_function\n",
    "\n",
    "def a2c_critic_loss(B):\n",
    "    \"\"\"\n",
    "    Defines a loss function for the critic that can access the bandit B while conforming to Keras's expected inputs (only y_true and y_pred)\n",
    "    \"\"\"\n",
    "    \n",
    "    def loss_function(y_true,y_pred): \n",
    "        return K.variable(B.critic_loss)\n",
    "\n",
    "    return loss_function\n",
    "\n",
    "def init_a2c(B, kwargs):\n",
    "    \"\"\"\n",
    "    Initializes an agent that implements the Advantage Actor-Critic (A2C) method\n",
    "    \"\"\"\n",
    "    B.N_arms = kwargs['N_arms'] \n",
    "    B.context = env.reset()\n",
    "    B.context = B.context.reshape(1,B.context.shape[0])\n",
    "    B.done = False\n",
    "    \n",
    "    B.Q = []\n",
    "    B.logP = [ [] ]\n",
    "    B.values = []\n",
    "    B.actions = []\n",
    "    B.policies = []\n",
    "    B.advantage = []\n",
    "    B.discount = kwargs['discount']\n",
    "    B.maxlen = kwargs['maxlen']\n",
    "    \n",
    "    \n",
    "    inputs = Input( [ env.observation_space.shape[0] ] )\n",
    "    shared = Dense(64, activation='relu')(inputs)\n",
    "    shared = Dense(128, activation='relu')(shared)\n",
    "    shared = Model(inputs, shared)\n",
    "    \n",
    "    actor_hidden = Dense(128, activation='relu')(shared.output)\n",
    "    actor_out = Dense(kwargs['N_arms'], activation='softmax')(actor_hidden)\n",
    "    B.actor = Model(shared.input, actor_out)\n",
    "    \n",
    "    critic_hidden = Dense(128, activation='relu')(shared.output)\n",
    "    critic_out = Dense(1, activation='softmax')(critic_hidden)\n",
    "    B.critic = Model(shared.input, critic_out)\n",
    "    \n",
    "    \n",
    "    B.optimizer =  RMSprop(lr=kwargs['lr'])\n",
    "    \n",
    "    B.action_pl = K.placeholder(shape=(None, B.N_arms))\n",
    "    B.advantages_pl = K.placeholder(shape=(None,))\n",
    "    B.discounted_r = K.placeholder(shape=(None,))\n",
    "    \n",
    "    B.weighted_actions = K.sum(B.action_pl * B.actor.output, axis=1)\n",
    "    B.eligibility = K.log(B.weighted_actions + 1e-10) * K.stop_gradient(B.advantages_pl)\n",
    "    B.entropy = K.sum(B.actor.output * K.log(B.actor.output + 1e-10), axis=1)\n",
    "    B.loss = 0.001 * B.entropy - K.sum(B.eligibility)\n",
    "    \n",
    "    updates = B.optimizer.get_updates(B.actor.trainable_weights, [], B.loss)\n",
    "    B.actor_opt = K.function([B.actor.input, B.action_pl, B.advantages_pl], B.actor.output, updates=updates)\n",
    "    \n",
    "    B.critic_loss = K.mean( K.square( B.discounted_r - B.critic.output ) )\n",
    "    updates = B.optimizer.get_updates(B.critic.trainable_weights, [], B.critic_loss)\n",
    "    B.critic_opt = K.function([B.critic.input, B.discounted_r], B.critic.output, updates=updates)\n",
    "    \n",
    "    init = tf.initialize_all_variables()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    \n",
    "    B.history = []\n",
    "    B.logP = []\n",
    "    B.entropy = 0\n",
    "    return\n",
    "\n",
    "\n",
    "def a2c(B):\n",
    "    \"\"\"\n",
    "    Selects policy from a weighted distribution calculated by the agent from the current state \n",
    "    \"\"\"\n",
    "    value = B.critic.predict( B.context )[0][0]\n",
    "    policy = B.actor.predict( B.context )[0] \n",
    "    B.values.append( value )\n",
    "    B.policies.append( policy )\n",
    "    return np.random.choice( B.N_arms, p=policy )\n",
    "\n",
    "\n",
    "def update_a2c(B, arm, reward, observation):\n",
    "    \"\"\"\n",
    "    Updates the agent based on the most recent action and reward using the Advantage Actor-Critic (A2C) method\n",
    "    \"\"\"\n",
    "    observation = observation.reshape(1, observation.shape[0])\n",
    "    B.history.append( (B.context, arm, reward, observation) )\n",
    "    \n",
    "    B.logP.append( -log( B.policies[-1][arm] + 1e-10 ) )\n",
    "    \n",
    "    if B.done or len(B.history) == B.maxlen:\n",
    "        N = len(B.history)\n",
    "        B.R = [ 0 for n in range(N) ]\n",
    "        B.R[-1] = B.history[-1][2]\n",
    "        for i in range(N-1, 0, -1):\n",
    "            B.R[i-1] = B.history[i-1][2] + B.R[i] * B.discount\n",
    "        \n",
    "        B.R = np.asarray(B.R, dtype=np.float32)\n",
    "        B.discounted_r = tf.convert_to_tensor(B.R)\n",
    "        \n",
    "        B.action_pl = tf.convert_to_tensor( B.policies[-1].reshape( (1,2) ) )\n",
    "        B.advantages_pl = tf.convert_to_tensor( (B.R - np.asarray(B.values)).astype(np.float32) )\n",
    "        \n",
    "        weighted_actions = K.sum(B.action_pl * B.actor.output, axis=1)\n",
    "        eligibility = K.log(weighted_actions + 1e-10) * K.stop_gradient(B.advantages_pl)\n",
    "        entropy = K.sum(B.actor.output * K.log(B.actor.output + 1e-10), axis=1)\n",
    "        B.loss = 0.001 * entropy - K.sum(eligibility)\n",
    "        \n",
    "        c2t = lambda x: tf.convert_to_tensor(x)\n",
    "        \n",
    "        states, actions, rewards, next_states = zip(*B.history)\n",
    "        states = c2t( np.squeeze( np.asarray(states,dtype=np.float32) ) )\n",
    "        actions = c2t( np.squeeze( np.asarray(actions,dtype=np.float32) ) )\n",
    "        rewards = c2t( np.squeeze( np.asarray(rewards,dtype=np.float32) ) )\n",
    "        \n",
    "        B.actor_opt( [states, actions, rewards] )\n",
    "        B.critic_opt( [states, B.discounted_r] )\n",
    "        \n",
    "        B.history = []\n",
    "        B.logP = []\n",
    "        B.values = []\n",
    "        B.actions = []\n",
    "        B.policies = []\n",
    "        \n",
    "    B.context = observation\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell uses the functions above to create an A2C bandit and trains it on the defined environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "\n",
    "kwargs = { \"N_arms\": env.action_space.n, \"discount\": 0.99, \"lr\": 0.001, \"layer_size\": [256], \"maxlen\": 20000}\n",
    "\n",
    "# Creates a multi-armed bandit using the defined A2C functions and parameters\n",
    "B = Bandit(env, a2c, update_a2c, init_a2c, kwargs )\n",
    "\n",
    "\n",
    "\n",
    "# Initialize variables to track iterations, rewards and weights\n",
    "counter = 0\n",
    "best_return = -200\n",
    "rewards = []\n",
    "all_weights = []\n",
    "\n",
    "\n",
    "# Sets the number of iterations and interval to print average reward (never prints if set to None)\n",
    "N_iterations = 10000\n",
    "interval = 100\n",
    "\n",
    "# Toggles whether the animation of the environment is rendered during training\n",
    "render = False\n",
    "\n",
    "for i in range(N_iterations):\n",
    "    done = False\n",
    "    counter += 1\n",
    "    if interval is not None and counter%interval == 0:\n",
    "        print( counter, \"\\t\", sum(rewards[-interval:])/float(interval) )\n",
    "    total_return = 0\n",
    "    env.reset()\n",
    "    j = 0\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        arm = B.policy(B)\n",
    "        observation, reward, done, info = env.step(arm)\n",
    "        reward = 1 if reward > 0 else -1 if reward < 0 else 0\n",
    "        B.done = done\n",
    "        j += 1\n",
    "        B.update(B, arm, reward, observation )\n",
    "        total_return += reward\n",
    "        if done:\n",
    "            rewards.append( total_return )\n",
    "            env.reset()\n",
    "            if total_return > best_return:\n",
    "                best_return = total_return\n",
    "            break\n",
    "env.close()\n",
    "\n",
    "print( \"Finished after \", counter, \" episodes with a top score of\", best_return )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
